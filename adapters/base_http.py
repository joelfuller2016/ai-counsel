"""Base HTTP adapter with request/retry management."""
import asyncio
import json
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple

import httpx
from tenacity import (retry, retry_if_exception, stop_after_attempt,
                      wait_exponential)

# Configure progress logger for HTTP adapter debugging
progress_logger = logging.getLogger("ai_counsel.progress")
if not progress_logger.handlers:
    # Log to both console and dedicated progress file
    project_dir = Path(__file__).parent.parent
    progress_file = project_dir / "deliberation_progress.log"
    progress_handler = logging.FileHandler(
        progress_file, mode="a", encoding="utf-8"
    )
    progress_handler.setFormatter(logging.Formatter(
        "%(asctime)s | %(levelname)s | %(message)s"
    ))
    progress_logger.addHandler(progress_handler)
    progress_logger.setLevel(logging.DEBUG)


def is_retryable_http_error(exception):
    """
    Determine if an HTTP error should be retried.

    Retries on:
    - 5xx server errors
    - 429 rate limit errors
    - Network errors (connection, timeout)

    Does NOT retry on:
    - 4xx client errors (bad request, auth, etc.)

    Args:
        exception: The exception to check

    Returns:
        bool: True if the error should be retried
    """
    if isinstance(exception, httpx.HTTPStatusError):
        # Retry on 5xx server errors and 429 rate limit
        return (
            exception.response.status_code >= 500
            or exception.response.status_code == 429
        )

    # Retry on network errors
    return isinstance(
        exception, (httpx.ConnectError, httpx.TimeoutException, httpx.NetworkError)
    )


class BaseHTTPAdapter(ABC):
    """
    Abstract base class for HTTP API adapters.

    Handles HTTP requests, timeout management, retry logic with exponential backoff,
    and error handling. Subclasses must implement build_request() and parse_response()
    for API-specific logic.

    Example:
        class MyAdapter(BaseHTTPAdapter):
            def build_request(self, model, prompt):
                return ("/api/generate", {"Content-Type": "application/json"}, {"prompt": prompt})

            def parse_response(self, response_json):
                return response_json["text"]

        adapter = MyAdapter(base_url="http://localhost:8080", timeout=60)
        result = await adapter.invoke(prompt="Hello", model="my-model")
    """

    # Default prompt length limits per HTTP adapter type (in characters)
    # These are conservative limits to prevent API rejection errors
    DEFAULT_PROMPT_LIMITS: dict[str, int] = {
        "ollama": 50_000,      # Local models typically have smaller context
        "lmstudio": 50_000,    # Local models typically have smaller context
        "openrouter": 100_000, # OpenRouter supports various models
    }

    # Adapter name for logging and limit lookup (subclasses should override)
    ADAPTER_NAME: str = "http"

    def __init__(
        self,
        base_url: str,
        timeout: int = 60,
        max_retries: int = 3,
        api_key: Optional[str] = None,
        headers: Optional[dict[str, str]] = None,
        max_prompt_length: Optional[int] = None,
    ):
        """
        Initialize HTTP adapter.

        Args:
            base_url: Base URL for API (e.g., "http://localhost:11434")
            timeout: Request timeout in seconds (default: 60)
            max_retries: Maximum retry attempts for transient failures (default: 3)
            api_key: Optional API key for authentication
            headers: Optional default headers to include in all requests
            max_prompt_length: Maximum prompt length in characters. If not specified,
                uses adapter-specific default from DEFAULT_PROMPT_LIMITS.
        """
        self.base_url = base_url.rstrip("/")  # Remove trailing slash
        self.timeout = timeout
        self.max_retries = max_retries
        self.api_key = api_key
        self.default_headers = headers or {}
        self._max_prompt_length = max_prompt_length

    @property
    def max_prompt_length(self) -> int:
        """
        Get the maximum prompt length for this adapter.

        Returns configured value if set, otherwise looks up default by adapter name.
        Falls back to 100,000 characters if no specific default exists.
        """
        if self._max_prompt_length is not None:
            return self._max_prompt_length
        return self.DEFAULT_PROMPT_LIMITS.get(self.ADAPTER_NAME, 100_000)

    def validate_prompt_length(self, prompt: str) -> bool:
        """
        Validate that prompt length is within allowed limits.

        Args:
            prompt: The prompt text to validate

        Returns:
            True if prompt is valid length, False if too long
        """
        return len(prompt) <= self.max_prompt_length

    @abstractmethod
    def build_request(
        self, model: str, prompt: str
    ) -> Tuple[str, dict[str, str], dict]:
        """
        Build API-specific request components.

        Args:
            model: Model identifier
            prompt: The prompt to send

        Returns:
            Tuple of (endpoint, headers, body):
            - endpoint: Full URL path (e.g., "/api/generate")
            - headers: Request headers dict
            - body: Request body dict (will be JSON-encoded)
        """
        pass

    @abstractmethod
    def parse_response(self, response_json: dict) -> str:
        """
        Parse API-specific response to extract model output.

        Args:
            response_json: Parsed JSON response from API

        Returns:
            Extracted model response text
        """
        pass

    async def invoke(
        self,
        prompt: str,
        model: str,
        context: Optional[str] = None,
        is_deliberation: bool = True,
        working_directory: Optional[str] = None,
        reasoning_effort: Optional[str] = None,
        timeout_override: Optional[int] = None,
    ) -> str:
        """
        Invoke the HTTP API with the given prompt and model.

        Args:
            prompt: The prompt to send to the model
            model: Model identifier
            context: Optional additional context to prepend to prompt
            is_deliberation: Whether this is part of a deliberation (unused for HTTP,
                           kept for API compatibility with BaseCLIAdapter)
            working_directory: Unused for HTTP adapters (kept for API compatibility)
            reasoning_effort: Unused for HTTP adapters (kept for API compatibility)
            timeout_override: Optional model-specific timeout in seconds. If provided,
                overrides the adapter's default timeout. Useful for reasoning models
                that require longer response times.

        Returns:
            Parsed response from the model

        Raises:
            TimeoutError: If request exceeds timeout
            httpx.HTTPStatusError: If API returns error status
            RuntimeError: If retries exhausted
        """
        # Use model-specific timeout if provided, otherwise use adapter default
        effective_timeout = timeout_override if timeout_override is not None else self.timeout
        # Build full prompt
        full_prompt = prompt
        if context:
            full_prompt = f"{context}\n\n{prompt}"

        # Validate prompt length
        if not self.validate_prompt_length(full_prompt):
            raise ValueError(
                f"Prompt too long ({len(full_prompt):,} characters). "
                f"Maximum allowed for {self.ADAPTER_NAME}: {self.max_prompt_length:,} characters. "
                f"Consider reducing context or breaking into smaller chunks. "
                f"This validation prevents API rejection errors."
            )

        # Get request components from subclass
        endpoint, headers, body = self.build_request(model, full_prompt)

        # Build full URL
        full_url = f"{self.base_url}{endpoint}"

        # Log request details for debugging
        logger = logging.getLogger(__name__)
        body_str = json.dumps(body, default=str)

        # Enhanced progress logging
        progress_logger.info(f"[START] HTTP REQUEST | Model: {model} | URL: {full_url}")
        progress_logger.debug(f"   API Key present: {bool(self.api_key)}")
        progress_logger.debug(f"   Prompt length: {len(full_prompt)} chars")
        progress_logger.debug(f"   Body size: {len(body_str)} bytes")
        progress_logger.debug(f"   Headers: {list(headers.keys())}")
        timeout_info = f" (model override)" if timeout_override else ""
        progress_logger.debug(f"   Timeout: {effective_timeout}s{timeout_info}")

        start_time = datetime.now()

        # Execute request with retry logic
        try:
            response_json = await self._execute_request_with_retry(
                url=full_url, headers=headers, body=body, timeout=effective_timeout
            )
            elapsed = (datetime.now() - start_time).total_seconds()
            progress_logger.info(f"[SUCCESS] HTTP REQUEST | Model: {model} | Time: {elapsed:.2f}s")
            progress_logger.debug(f"   Response keys: {list(response_json.keys()) if isinstance(response_json, dict) else 'N/A'}")
            return self.parse_response(response_json)

        except asyncio.TimeoutError:
            elapsed = (datetime.now() - start_time).total_seconds()
            progress_logger.error(f"[TIMEOUT] HTTP REQUEST | Model: {model} | Time: {elapsed:.2f}s")
            raise TimeoutError(f"HTTP request timed out after {effective_timeout}s")

        except Exception as e:
            elapsed = (datetime.now() - start_time).total_seconds()
            progress_logger.error(f"[ERROR] HTTP REQUEST FAILED | Model: {model} | Time: {elapsed:.2f}s | Error: {type(e).__name__}: {str(e)[:200]}")
            raise

    async def _execute_request_with_retry(
        self, url: str, headers: dict[str, str], body: dict, timeout: Optional[int] = None
    ) -> dict:
        """
        Execute HTTP POST request with retry logic.

        Uses tenacity for exponential backoff retry on:
        - 5xx server errors
        - 429 rate limit errors
        - Network errors (connection, timeout)

        Does NOT retry on:
        - 4xx client errors (bad request, auth, etc.)

        Args:
            url: Full request URL
            headers: Request headers
            body: Request body (will be JSON-encoded)
            timeout: Request timeout in seconds (uses adapter default if not specified)

        Returns:
            Parsed JSON response

        Raises:
            httpx.HTTPStatusError: On HTTP error (after retries exhausted for 5xx)
            httpx.NetworkError: On network error (after retries exhausted)
        """
        effective_timeout = timeout if timeout is not None else self.timeout

        @retry(
            stop=stop_after_attempt(self.max_retries),
            wait=wait_exponential(multiplier=1, min=1, max=10),
            retry=retry_if_exception(is_retryable_http_error),
            reraise=True,
        )
        async def _make_request():
            async with httpx.AsyncClient(timeout=effective_timeout) as client:
                progress_logger.debug(f"   [POST] Making request to {url}")
                response = await client.post(url, headers=headers, json=body)
                progress_logger.debug(f"   [RESPONSE] Status: {response.status_code}")

                # Log error response body for 4xx errors (helps debugging)
                if 400 <= response.status_code < 500:
                    try:
                        error_body = response.json()
                        progress_logger.error(
                            f"   [HTTP_ERROR] {response.status_code}: {json.dumps(error_body, indent=2)}"
                        )
                    except Exception:
                        progress_logger.error(
                            f"   [HTTP_ERROR] {response.status_code} body: {response.text[:500]}"
                        )

                response.raise_for_status()  # Raise for 4xx/5xx
                return response.json()

        return await _make_request()
